{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marked_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4dc63e0bf8248829c644bda23aec29f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_963176571dc8414e8fb777e0d220282c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61521c3a98b2449192b7676750ef5414",
              "IPY_MODEL_84ae162b6c194258a2c031d0064870dd"
            ]
          }
        },
        "963176571dc8414e8fb777e0d220282c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61521c3a98b2449192b7676750ef5414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_21d093dd96954aa28dca568ead9ca3b4",
            "_dom_classes": [],
            "description": "Iteration: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1344,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1344,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_54976d265bb14833ab8b37afd213b3f5"
          }
        },
        "84ae162b6c194258a2c031d0064870dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7de933c6c5cb4b8e85a9e2d630f3e4c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1344/1344 [03:34&lt;00:00,  6.26it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dbc7a0892c03409a8a478c101b149c9e"
          }
        },
        "21d093dd96954aa28dca568ead9ca3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "54976d265bb14833ab8b37afd213b3f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7de933c6c5cb4b8e85a9e2d630f3e4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dbc7a0892c03409a8a478c101b149c9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbougha/DeepLearning.ai-Summary/blob/master/Marked_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hFm6ylUn8oSu",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoV-0WLZ8mBP",
        "colab_type": "code",
        "outputId": "8bbf1bf0-815e-49fb-e23e-ba9a19025950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpbpoe4yygpU",
        "colab_type": "text"
      },
      "source": [
        "**Firstly**, we need to set up Colab TPU running environment, verify a TPU device is succesfully connected and upload credentials to TPU for GCS bucket usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clxY8P9cGCXF",
        "colab_type": "code",
        "outputId": "a903517d-fc10-4d00-d2d2-24bff10675a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import sys\n",
        "#username='??,?'#@param {type:\"string\"}\n",
        "#password='???'#@param {type:\"string\"}\n",
        "#!test -d MarkedBERT || git clone https://{username}:{password}@github.com/{username}/MarkedBERT.git\n",
        "if not 'MarkedBERT' in sys.path:\n",
        "  sys.path += ['MarkedBERT']\n",
        "print (sys.path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', 'MarkedBERT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0eAqa5g13Y6",
        "outputId": "1120e893-4ba5-43ed-b32e-b6ca8044c551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        }
      },
      "source": [
        "!pip install -r MarkedBERT/requirements_colab.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly==2.2.0-dev20200311 in /usr/local/lib/python3.6/dist-packages (from -r MarkedBERT/requirements_colab.txt (line 1)) (2.2.0.dev20200311)\n",
            "Requirement already satisfied: transformers>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from -r MarkedBERT/requirements_colab.txt (line 2)) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (2.3.0.dev2020041201)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: tb-nightly<2.3.0a0,>=2.2.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (2.2.0a20200324)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.28.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (4.38.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.0.40)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (1.12.38)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (46.1.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.6.0.post3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (1.15.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.9.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers>=2.5.1->-r MarkedBERT/requirements_colab.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.3.0a0,>=2.2.0a0->tf-nightly==2.2.0-dev20200311->-r MarkedBERT/requirements_colab.txt (line 1)) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "87e94b77-6a85-4e60-baca-6abe13db7a6d",
        "id": "RCbDvDYx13ZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-dev20200311'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdUFaYR4wwZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pt-66aNExJMK"
      },
      "source": [
        "# DATA FILES NEED TO BE IN GCS BUCKET SO THAT THE TPU CAN ACCESS IT \n",
        "THE FIRST RUN: THE TPU HAS NO ACCESS PERMISSIONS TO THE BUCKET SO U NEED TO CHANGE THE PERMISSIONS OF YOUR BUCKET MANUALLY TO ADD THE TPU ( THE NAME WILL BE IN THE EXCEPTION TEXT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJvKZN02w2j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['COLAB_SKIP_TPU_AUTH'] = '1'\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FaSBpUMw6dZ",
        "colab_type": "code",
        "outputId": "2325e81a-7871-43fa-dcde-a638f9c70300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "project_id=\"windy-impact-273008\"#@param {type:\"string\"}\n",
        "bucket_name=\"mbougha2\"#@param {type:\"string\"}\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJEo5ZakzPNo",
        "colab_type": "text"
      },
      "source": [
        "**Thirdly**, prepare for training:\n",
        "\n",
        "* Specify training data.\n",
        "* Specify BERT pretrained model\n",
        "* Specify GS bucket, create output directory for model checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NsGKuHdzWzZ",
        "colab_type": "code",
        "outputId": "8fa2768e-9564-4cb0-df2b-05877fe248d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "MODEL_TYPE = \"bert\" #@param {type:\"string\"}\n",
        "MODEL_NAME = \"bert-base-uncased\" #@param {type:\"string\"}\n",
        "\n",
        "OUTPUT_DIR = \"gs://mbougha2/output_dir\" #@param {type:\"string\"}\n",
        "assert OUTPUT_DIR, 'Must specify an existing GCS bucket name'\n",
        "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Now we need to specify the input data dir. Should contain the .tfrecord files \n",
        "# and the supporting query-docids mapping files.\n",
        "DATA_DIR = \"gs://mbougha2/trec_test_set\" #@param {type:\"string\"}\n",
        "print('***** Data directory: {} *****'.format(DATA_DIR))\n",
        "\n",
        "FILE_NAME= \"dataset_trec_base.tf\" #@param {type:\"string\"}\n",
        "SET_NAME= \"trec\" #@param {type:\"string\"}\n",
        "\n",
        "# need to mount your drive and put the path to the directory containing the .h5 file\n",
        "CHKPT_PATH=\"/content/drive/My Drive/DeepL/\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://mbougha2/output_dir *****\n",
            "***** Data directory: gs://mbougha2/trec_test_set *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI_d4sfn0g-R",
        "colab_type": "text"
      },
      "source": [
        "**Now, we can start training/evaluating**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jBv9QA3X13Zj",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "import collections\n",
        "import datetime\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from absl import app, flags, logging\n",
        "from tqdm import trange\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import (\n",
        "    TF2_WEIGHTS_NAME,\n",
        "    BertConfig,\n",
        "    BertTokenizer,\n",
        "    DistilBertConfig,\n",
        "    DistilBertTokenizer,\n",
        "    RobertaConfig,\n",
        "    RobertaTokenizer,\n",
        "    TFBertForSequenceClassification,\n",
        "    TFDistilBertForSequenceClassification,\n",
        "    TFRobertaForSequenceClassification,\n",
        ")\n",
        "#local modules\n",
        "from Modeling import get_dataset, create_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "37v3LH-I13Zn",
        "colab": {}
      },
      "source": [
        "ALL_MODELS = sum(\n",
        "    (tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, RobertaConfig, DistilBertConfig)), ()\n",
        ")\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"bert\": (BertConfig, TFBertForSequenceClassification, BertTokenizer),\n",
        "    \"roberta\": (RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer),\n",
        "    \"distilbert\": (DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer),\n",
        "}\n",
        "\n",
        "#Arguments \n",
        "args=dict()\n",
        "\n",
        "args[\"data_dir\"]= DATA_DIR\n",
        "args[\"model_type\"]=MODEL_TYPE\n",
        "args[\"model_name_or_path\"]=MODEL_NAME\n",
        "args[\"output_dir\"]=OUTPUT_DIR\n",
        "args[\"transformer_checkpoints\"]= CHKPT_PATH\n",
        "args[\"max_seq_length\"]=512\n",
        "args[\"tpu\"]= f\"grpc://{os.environ['COLAB_TPU_ADDR']}\"\n",
        "args[\"do_train\"]= False\n",
        "args['do_eval']= True\n",
        "args[\"per_device_train_batch_size\"]= 16\n",
        "args['per_device_eval_batch_size']= 4\n",
        "args[\"max_steps\"]=100000\n",
        "args[\"warmup_steps\"]=10000\n",
        "args[\"learning_rate\"]=3e-6\n",
        "args[\"adam_epsilon\"]=1e-8\n",
        "args[\"logging_steps\"]=100\n",
        "args[\"seed\"]=42\n",
        "args[\"max_grad_norm\"]=1.0\n",
        "args[\"save_steps\"]=5000\n",
        "args[\"overwrite_output_dir\"]=False\n",
        "args[\"fp16\"]=False\n",
        "args[\"no_cuda\"]=False\n",
        "args['gpus']=None\n",
        "args['config_name']=None\n",
        "args['cache_dir']=None\n",
        "args['tokenizer_name']=None\n",
        "args['num_tpu_cores']=8\n",
        "args['do_predict']=False\n",
        "args['evaluate_during_training']=False\n",
        "args['do_lower_case']=False\n",
        "args['num_train_epochs']=1\n",
        "args['overwrite_cache']=False\n",
        "args['eval_all_checkpoints']=False\n",
        "args[\"msmarco_output\"]= True\n",
        "args[\"num_eval_docs\"] =1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h1BVqcDD13Zz",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(\n",
        "    args, strategy,  train_dataset, model, num_train_examples, train_batch_size\n",
        "):\n",
        "    if args[\"max_steps\"] > 0:\n",
        "        num_train_steps = args[\"max_steps\"] \n",
        "        args[\"num_train_epochs\"] = 1 # only consider the case where max_steps < one_epoch_steps\n",
        "    else:\n",
        "        num_train_steps = (\n",
        "            math.ceil(num_train_examples / train_batch_size)\n",
        "            * args[\"num_train_epochs\"]\n",
        "        )\n",
        "\n",
        "    with strategy.scope():\n",
        "        loss_fct = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        "        optimizer = create_optimizer(args[\"learning_rate\"], num_train_steps, args[\"warmup_steps\"])\n",
        "\n",
        "        if args[\"fp16\"]:\n",
        "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \"dynamic\")\n",
        "\n",
        "        loss_metric = tf.keras.metrics.Mean(name=\"loss\", dtype=tf.float32)\n",
        "        \n",
        "    logging.info(\"***** Running training *****\")\n",
        "    logging.info(\"  Num examples = %d\", num_train_examples)\n",
        "    logging.info(\"  Num Epochs = %d\", args[\"num_train_epochs\"])\n",
        "    logging.info(\"  Instantaneous batch size per device = %d\", args[\"per_device_train_batch_size\"])\n",
        "    logging.info(\n",
        "        \"  Total train batch size (w. parallel, distributed ) = %d\",\n",
        "        train_batch_size,\n",
        "    )\n",
        "\n",
        "    logging.info(\"  Total training steps = %d\", num_train_steps)\n",
        "\n",
        "    model.summary()    \n",
        "\n",
        "    @tf.function\n",
        "    def train_step(train_features, train_labels):\n",
        "        def step_fn(train_features, train_labels):\n",
        "            inputs = {\"attention_mask\": train_features[\"attention_mask\"], \"training\": True}\n",
        "\n",
        "            if args[\"model_type\"] != \"distilbert\":\n",
        "                inputs[\"token_type_ids\"] = (\n",
        "                    train_features[\"token_type_ids\"] if args[\"model_type\"] in [\"bert\", \"xlnet\"] else None\n",
        "                )\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(train_features[\"input_ids\"], **inputs)[0]\n",
        "                cross_entropy = loss_fct(train_labels, logits) #per_example_losses\n",
        "                loss = tf.reduce_sum(cross_entropy) * (1.0 / train_batch_size) #per_replica_loss\n",
        "                if args['fp16']:\n",
        "                    scaled_loss = optimizer.get_scaled_loss(loss)\n",
        "\n",
        "            if args['fp16']:\n",
        "              scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
        "              grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
        "            else:\n",
        "              grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            return cross_entropy\n",
        "\n",
        "        per_example_losses = strategy.experimental_run_v2(step_fn, args=(train_features, train_labels))\n",
        "        sum_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\n",
        "        \n",
        "        return sum_loss / train_batch_size #loss over replicas\n",
        "\n",
        "    current_time = datetime.datetime.now()\n",
        "\n",
        "    # Train or resume training from chkpt\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    current_step = 0\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Check if continuing training from a checkpoint\n",
        "        checkpoint = tf.train.Checkpoint(step=tf.Variable(0), model=model, optimizer=optimizer)\n",
        "        manager = tf.train.CheckpointManager(checkpoint, f'{args[\"output_dir\"]}/tf_ckpts', max_to_keep=3)\n",
        "        latest_checkpoint_file = manager.latest_checkpoint\n",
        "        if latest_checkpoint_file:\n",
        "          logging.info(\n",
        "              'Checkpoint file %s found and restoring from '\n",
        "              'checkpoint', latest_checkpoint_file)\n",
        "          checkpoint.restore(latest_checkpoint_file)\n",
        "          logging.info('Loading from checkpoint file completed')\n",
        "    if latest_checkpoint_file:\n",
        "        current_step = optimizer.iterations.numpy()\n",
        "        logging.info(\n",
        "              'Resume training from step %s', current_step )\n",
        "    print(\"\\ncurrent step = \", current_step)\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    epoch_iterator = trange(\n",
        "        epochs_trained, int(args[\"num_train_epochs\"]), desc=\"Epoch\")\n",
        "    global_step = current_step \n",
        "    # global_step without gradient_accumul ==step (useless) \n",
        "    #if multiple epochs global_step may contain steps from multiple epochs, \n",
        "    #step should be step_in_current_epoch, epochs_trained calculated\n",
        "\n",
        "    for epoch in epoch_iterator:\n",
        "        train_iterator = tqdm(train_dataset, total=num_train_steps, desc=\"Iteration\")\n",
        "\n",
        "        with strategy.scope():\n",
        "            for step, (train_features, train_labels) in enumerate(train_iterator):\n",
        "                # Skip past any already trained steps if resuming training\n",
        "                if current_step > 0:\n",
        "                    current_step -= 1\n",
        "                    continue\n",
        "\n",
        "                loss = train_step(train_features, train_labels)\n",
        "                    \n",
        "                loss_metric(loss)\n",
        "\n",
        "                global_step += 1\n",
        "                checkpoint.step.assign_add(1)\n",
        "\n",
        "                if args[\"logging_steps\"] > 0 and global_step % args[\"logging_steps\"] == 0:\n",
        "                        # Log metrics\n",
        "                        lr = optimizer.learning_rate\n",
        "                        learning_rate = lr(step)\n",
        "                        \n",
        "                        logging_loss = loss_metric.result()\n",
        "\n",
        "                        train_iterator.set_postfix(loss=logging_loss.numpy(), step=global_step, lr=learning_rate.numpy())\n",
        "\n",
        "                if args[\"save_steps\"] > 0 and global_step % args[\"save_steps\"] == 0:\n",
        "                        ## TF2 checkpoint for training \n",
        "                        save_path = manager.save()\n",
        "                        logging.info(\"Saved checkpoint for step %s: %s\",global_step,save_path)\n",
        "                        \n",
        "                if args[\"save_steps\"] > 0 and global_step % (args[\"save_steps\"]*10) == 0:\n",
        "                        # Save model checkpoint\n",
        "                        output_dir = os.path.join(args[\"transformer_checkpoints\"], f\"checkpoint-{global_step}\")\n",
        "\n",
        "                        if not os.path.exists(output_dir):\n",
        "                            os.makedirs(output_dir)\n",
        "\n",
        "                        model.save_pretrained(output_dir)\n",
        "                        \n",
        "                        logging.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "                if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
        "                    train_iterator.close()\n",
        "                    break\n",
        "        if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
        "            epoch_iterator.close()\n",
        "            break\n",
        "\n",
        "        epoch_iterator.write(f\"loss epoch {epoch + 1}: {loss_metric.result()}\")\n",
        "\n",
        "        loss_metric.reset_states()\n",
        "\n",
        "    logging.info(\"  Training took time = {}\".format(datetime.datetime.now() - current_time))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TmBXcLBJrgWT",
        "colab": {}
      },
      "source": [
        "def evaluate(args, strategy, eval_dataset, trained_model, num_eval_examples,\n",
        "            eval_batch_size, global_step):\n",
        "\n",
        "    @tf.function\n",
        "    def eval_step(features, labels):\n",
        "      \"\"\"Computes predictions on distributed devices.\"\"\"\n",
        "\n",
        "      def _eval_step_fn(eval_features, labels):\n",
        "        \"\"\"Replicated predictions.\"\"\"\n",
        "        inputs = {\"attention_mask\": eval_features[\"attention_mask\"], \"training\": False}\n",
        "\n",
        "        if args[\"model_type\"] != \"distilbert\":\n",
        "            inputs[\"token_type_ids\"] = (\n",
        "                eval_features[\"token_type_ids\"] if args[\"model_type\"] in [\"bert\", \"xlnet\"] else None\n",
        "            )\n",
        "\n",
        "        logits = trained_model(eval_features[\"input_ids\"], **inputs)[0]\n",
        "\n",
        "        return logits, labels, eval_features['q_id'], eval_features['d_id']\n",
        "\n",
        "      preds, labels, q_id, d_id = strategy.experimental_run_v2(\n",
        "          _eval_step_fn, args=(features, labels))\n",
        "      # outputs: current batch logits as a tuple of shard logits\n",
        "      preds = tf.nest.map_structure(strategy.experimental_local_results,\n",
        "                                      preds)\n",
        "      labels = tf.nest.map_structure(strategy.experimental_local_results, labels)\n",
        "      q_id = tf.nest.map_structure(strategy.experimental_local_results,\n",
        "                                      q_id)\n",
        "      d_id = tf.nest.map_structure(strategy.experimental_local_results,\n",
        "                                      d_id)\n",
        "      return preds, labels, q_id, d_id\n",
        "\n",
        "    \n",
        "    #start eval\n",
        "    num_eval_steps = (\n",
        "            math.ceil(num_eval_examples / eval_batch_size)\n",
        "        )\n",
        "    logging.info(\"***** Running evaluation *****\")\n",
        "    logging.info(\"  Num examples = %d\", num_eval_examples)\n",
        "    logging.info(\"  Instantaneous batch size per device = %d\", \n",
        "                 args[\"per_device_eval_batch_size\"])\n",
        "    logging.info(\n",
        "        \"  Total train batch size (w. parallel, distributed ) = %d\",\n",
        "        eval_batch_size,\n",
        "    )\n",
        "\n",
        "    logging.info(\"  Total evaluation steps = %d\", num_eval_steps)\n",
        "    eval_iterator = tqdm(eval_dataset, total=num_eval_steps, \n",
        "                                   desc=\"Iteration\")\n",
        "\n",
        "    preds = None\n",
        "    golds = None\n",
        "    qids = None\n",
        "    dids= None\n",
        "    msmarco_file = tf.io.gfile.GFile( f\"{args['output_dir']}/predictions_{SET_NAME}_{global_step}.tsv\", 'w')\n",
        "    for step, (eval_features, eval_labels) in enumerate(eval_iterator):\n",
        "        with strategy.scope():\n",
        "            outputs, labels, q_ids, d_ids  = eval_step(eval_features, eval_labels)\n",
        "        \n",
        "        for i in range(args['n_device']):\n",
        "            if preds is None:\n",
        "                preds = outputs[i].numpy()[:,1]\n",
        "                golds = labels[i].numpy()\n",
        "                qids = q_ids[i].numpy()\n",
        "                dids = d_ids[i].numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, outputs[i].numpy()[:,1], axis=0)\n",
        "                golds = np.append(golds, labels[i].numpy(), axis=0)\n",
        "                qids = np.append(qids, q_ids[i].numpy(), axis=0)\n",
        "                dids = np.append(dids, d_ids[i].numpy(), axis=0)\n",
        "                \n",
        "        for qid,did,pred,label in zip(qids,dids,preds,golds):\n",
        "              msmarco_file.write(\"\\t\".join((str(qid), str(did), str(pred), str(label))) + \"\\n\")\n",
        "        preds = None\n",
        "        golds = None\n",
        "        qids = None\n",
        "        dids= None\n",
        "    msmarco_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dgn4zafy13Zr",
        "colab": {}
      },
      "source": [
        "def main(args):\n",
        "  #  logging.set_verbosity(logging.INFO)\n",
        "\n",
        "    if args[\"fp16\"]:\n",
        "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
        "\n",
        "    if args[\"tpu\"]:\n",
        "        print(args['tpu'])\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args[\"tpu\"])\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)       \n",
        "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "        args[\"n_device\"] = args[\"num_tpu_cores\"]\n",
        "\n",
        "    elif args[\"no_cuda\"]:\n",
        "        args[\"n_device\"] = 1\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "\n",
        "    elif args[\"gpus\"]:\n",
        "        if len(args[\"gpus\"].split(\",\")) > 1:\n",
        "            args[\"n_device\"] = len([f\"/gpu:{gpu}\" for gpu in args[\"gpus\"].split(\",\")])\n",
        "            strategy = tf.distribute.MirroredStrategy(devices=[f\"/gpu:{gpu}\" for gpu in args[\"gpus\"].split(\",\")])\n",
        "        else:\n",
        "            args[\"n_device\"] = len(args[\"gpus\"].split(\",\"))\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:\" + args[\"gpus\"].split(\",\")[0])\n",
        "\n",
        "    else:\n",
        "        devices = get_available_gpus()\n",
        "        logging.info(\"\\ndevices= %s \\n\", devices)\n",
        "        args[\"n_device\"] = len(devices)\n",
        "        strategy = tf.distribute.MirroredStrategy(devices=devices)\n",
        "    \n",
        "    logging.warning(\n",
        "        \"n_device: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args[\"n_device\"],\n",
        "        bool(args[\"n_device\"] > 1),\n",
        "        args[\"fp16\"],\n",
        "    )\n",
        "    \n",
        "    logging.info(\"\\nStrategy = %s\\n\",strategy)\n",
        "\n",
        "    num_labels = 2\n",
        "    tf.random.set_seed(args[\"seed\"])\n",
        "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
        "    config = config_class.from_pretrained(\n",
        "        args[\"config_name\"] if args[\"config_name\"] else args[\"model_name_or_path\"],\n",
        "        num_labels=num_labels,\n",
        "        cache_dir=args[\"cache_dir\"] if args[\"cache_dir\"] else None,\n",
        "    )\n",
        "\n",
        "    logging.info(\"Training/evaluation parameters %s\", args)\n",
        "    \n",
        "    # Training\n",
        "    if args[\"do_train\"]:\n",
        "        # tokenizer = tokenizer_class.from_pretrained(\n",
        "        #     args[\"tokenizer_name\"] if args[\"tokenizer_name\"] else args[\"model_name_or_path\"],\n",
        "        #     do_lower_case=args[\"do_lower_case\"],\n",
        "        #     cache_dir=args[\"cache_dir\"] if args[\"cache_dir\"] else None,\n",
        "        # )\n",
        "\n",
        "        with strategy.scope():\n",
        "            model = model_class.from_pretrained(\n",
        "                args[\"model_name_or_path\"],\n",
        "                from_pt=bool(\".bin\" in args[\"model_name_or_path\"]),\n",
        "                config=config,\n",
        "                cache_dir=args[\"cache_dir\"] if args[\"cache_dir\"] else None,\n",
        "            )\n",
        "            model.layers[-1].activation = tf.keras.activations.softmax\n",
        "\n",
        "        train_batch_size = args[\"per_device_train_batch_size\"] * max(1,args[\"n_device\"])\n",
        "\n",
        "        filename = tf.io.gfile.glob(f\"{args['data_dir']}/dataset_train_name_or_path\")\n",
        "        train_dataset, num_train_examples = get_dataset( filename,\n",
        "                train_batch_size, args[\"max_seq_length\"], is_training_set=True\n",
        "        )\n",
        "        train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "        train(\n",
        "            args,\n",
        "            strategy,\n",
        "            train_dataset,\n",
        "            model,\n",
        "            num_train_examples,\n",
        "            train_batch_size,\n",
        "        )\n",
        "\n",
        "        if not os.path.exists(args[\"transformer_checkpoints\"]):\n",
        "            os.makedirs(args[\"transformer_checkpoints\"])\n",
        "\n",
        "        logging.info(\"Saving model to %s\", args[\"transformer_checkpoints\"])\n",
        "\n",
        "        model.save_pretrained(args[\"transformer_checkpoints\"])\n",
        "        #tokenizer.save_pretrained(args[\"transformer_checkpoints\"])\n",
        "    # Evaluating\n",
        "    if args[\"do_eval\"]:\n",
        "        checkpoints = []\n",
        "        print(\"-------------------------\")\n",
        "        if args[\"eval_all_checkpoints\"]:\n",
        "            print(\"JE SUIS DS ALL CHECHPOINTS\\n\")\n",
        "            print(\"ARG_TRANSFO:\", args[\"transformer_checkpoints\"])\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c)\n",
        "                for c in sorted(\n",
        "                    glob.glob(args[\"transformer_checkpoints\"] + \"/**/\" + TF2_WEIGHTS_NAME, recursive=True),\n",
        "                    key=lambda f: int(\"\".join(filter(str.isdigit, f)) or -1),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if len(checkpoints) == 0:\n",
        "            print(\"last chech point\\n\")\n",
        "            # last checkpoint\n",
        "            checkpoints = list(\n",
        "                  os.path.dirname(c)\n",
        "                  for c in sorted(\n",
        "                      glob.glob(args[\"transformer_checkpoints\"] + \"/**/\" + TF2_WEIGHTS_NAME, recursive=True),\n",
        "                      key=lambda f: int(\"\".join(filter(str.isdigit, f)) or -1),\n",
        "                  )\n",
        "              )\n",
        "            print(\"MB ARG Transformers: \", args[\"transformer_checkpoints\"])\n",
        "            print(\"MB CHECK POINTS:\", checkpoints)\n",
        "            checkpoints = [checkpoints[-1]]\n",
        "            \n",
        "            #checkpoints.append(args[\"transformer_checkpoints\"])\n",
        "        \n",
        "        logging.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "            \n",
        "        eval_batch_size = args[\"per_device_eval_batch_size\"] * max(1,args[\"n_device\"])\n",
        "\n",
        "        filename = tf.io.gfile.glob(f\"{args['data_dir']}/{FILE_NAME}\")\n",
        "        \n",
        "        print(\"ARG DATA_DIR: \", args['data_dir'], \"FILE_NAME:\", FILE_NAME)\n",
        "        print(\"filename\", filename)\n",
        "        \n",
        "        eval_dataset, num_eval_examples = get_dataset(filename, eval_batch_size, args[\"max_seq_length\"])\n",
        "        print('num_eval_examples= ', num_eval_examples)\n",
        "        eval_dataset = strategy.experimental_distribute_dataset(eval_dataset)\n",
        "        \n",
        "        \n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if re.match(\".*checkpoint-[0-9]\", checkpoint) else \"final\"\n",
        "            print(\"global step= \", global_step)\n",
        "\n",
        "            with strategy.scope():\n",
        "                trained_model = model_class.from_pretrained(checkpoint)\n",
        "                trained_model.summary()\n",
        "        \n",
        "            out = evaluate(\n",
        "                args,\n",
        "                strategy,\n",
        "                eval_dataset,\n",
        "                trained_model,\n",
        "                num_eval_examples,\n",
        "                eval_batch_size,\n",
        "                global_step,\n",
        "            )\n",
        "        #trained_model.save_pretrained(args[\"transformer_checkpoints\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvAZ2AHC1KQi",
        "colab_type": "code",
        "outputId": "11fbb2bf-8d06-44da-ad70-7b355a6e0dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c4dc63e0bf8248829c644bda23aec29f",
            "963176571dc8414e8fb777e0d220282c",
            "61521c3a98b2449192b7676750ef5414",
            "84ae162b6c194258a2c031d0064870dd",
            "21d093dd96954aa28dca568ead9ca3b4",
            "54976d265bb14833ab8b37afd213b3f5",
            "7de933c6c5cb4b8e85a9e2d630f3e4c9",
            "dbc7a0892c03409a8a478c101b149c9e"
          ]
        }
      },
      "source": [
        "main(args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "grpc://10.45.58.50:8470\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.45.58.50:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.45.58.50:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "WARNING:absl:n_device: 8, distributed training: True, 16-bits training: False\n",
            "INFO:absl:\n",
            "Strategy = <tensorflow.python.distribute.tpu_strategy.TPUStrategy object at 0x7fb61ff3a630>\n",
            "\n",
            "INFO:absl:Training/evaluation parameters {'data_dir': 'gs://mbougha2/trec_test_set', 'model_type': 'bert', 'model_name_or_path': 'bert-base-uncased', 'output_dir': 'gs://mbougha2/output_dir', 'transformer_checkpoints': '/content/drive/My Drive/DeepL/', 'max_seq_length': 512, 'tpu': 'grpc://10.45.58.50:8470', 'do_train': False, 'do_eval': True, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 4, 'max_steps': 100000, 'warmup_steps': 10000, 'learning_rate': 3e-06, 'adam_epsilon': 1e-08, 'logging_steps': 100, 'seed': 42, 'max_grad_norm': 1.0, 'save_steps': 5000, 'overwrite_output_dir': False, 'fp16': False, 'no_cuda': False, 'gpus': None, 'config_name': None, 'cache_dir': None, 'tokenizer_name': None, 'num_tpu_cores': 8, 'do_predict': False, 'evaluate_during_training': False, 'do_lower_case': False, 'num_train_epochs': 1, 'overwrite_cache': False, 'eval_all_checkpoints': False, 'msmarco_output': True, 'num_eval_docs': 1000, 'n_device': 8}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-------------------------\n",
            "last chech point\n",
            "\n",
            "MB ARG Transformers: "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Evaluate the following checkpoints: ['/content/drive/My Drive/DeepL/checkpoint-100000']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " /content/drive/My Drive/DeepL/\n",
            "MB CHECK POINTS: ['/content/drive/My Drive/DeepL/checkpoint-100000']\n",
            "ARG DATA_DIR:  gs://mbougha2/trec_test_set FILE_NAME: dataset_trec_base.tf\n",
            "filename ['gs://mbougha2/trec_test_set/dataset_trec_base.tf']\n",
            "num_eval_examples=  43000\n",
            "global step=  100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:***** Running evaluation *****\n",
            "INFO:absl:  Num examples = 43000\n",
            "INFO:absl:  Instantaneous batch size per device = 4\n",
            "INFO:absl:  Total train batch size (w. parallel, distributed ) = 32\n",
            "INFO:absl:  Total evaluation steps = 1344\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4dc63e0bf8248829c644bda23aec29f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Iteration', max=1344, style=ProgressStyle(description_width='â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHoH3_qVJEOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}